{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, Reduction\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, GlobalAvgPool2D, LeakyReLU, Rescaling,\n",
    "    Conv2D, Dense, PReLU, Add, Input\n",
    ")\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow import GradientTape, concat, zeros, ones, reduce_mean, distribute\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from matplotlib.pyplot import subplots, savefig, title, xticks, yticks, show\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from tensorflow.config import experimental_connect_to_cluster\n",
    "from tensorflow.tpu.experimental import initialize_tpu_system\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.io.gfile import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the TFDS dataset we will be using\n",
    "DATASET = \"div2k/bicubic_x4\"\n",
    "\n",
    "# define the shard size and batch size\n",
    "SHARD_SIZE = 256\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "INFER_BATCH_SIZE = 8\n",
    "\n",
    "# dataset specs\n",
    "HR_SHAPE = [96, 96, 3]\n",
    "LR_SHAPE = [24, 24, 3]\n",
    "SCALING_FACTOR = 4\n",
    "\n",
    "# GAN model specs\n",
    "FEATURE_MAPS = 64\n",
    "RESIDUAL_BLOCKS = 16\n",
    "LEAKY_ALPHA = 0.2\n",
    "DISC_BLOCKS = 4\n",
    "\n",
    "# training specs\n",
    "PRETRAIN_LR = 1e-4\n",
    "FINETUNE_LR = 1e-5\n",
    "PRETRAIN_EPOCHS = 2500\n",
    "FINETUNE_EPOCHS = 2500\n",
    "STEPS_PER_EPOCH = 10\n",
    "\n",
    "# define the path to the dataset\n",
    "BASE_DATA_PATH = \"dataset\"\n",
    "DIV2K_PATH = os.path.join(BASE_DATA_PATH, \"div2k\")\n",
    "\n",
    "# define the path to the tfrecords for GPU training\n",
    "GPU_BASE_TFR_PATH = \"tfrecord\"\n",
    "GPU_DIV2K_TFR_TRAIN_PATH = os.path.join(GPU_BASE_TFR_PATH, \"train\")\n",
    "GPU_DIV2K_TFR_TEST_PATH = os.path.join(GPU_BASE_TFR_PATH, \"test\")\n",
    "\n",
    "# path to our base output directory\n",
    "BASE_OUTPUT_PATH = \"outputs\"\n",
    "\n",
    "# GPU training SRGAN model paths\n",
    "GPU_PRETRAINED_GENERATOR_MODEL = os.path.join(BASE_OUTPUT_PATH,\n",
    "    \"models\", \"pretrained_generator\")\n",
    "GPU_GENERATOR_MODEL = os.path.join(BASE_OUTPUT_PATH, \"models\",\n",
    "    \"generator\")\n",
    "\n",
    "# define the path to the inferred images and to the grid image\n",
    "BASE_IMAGE_PATH = os.path.join(BASE_OUTPUT_PATH, \"images\")\n",
    "GRID_IMAGE_PATH = os.path.join(BASE_IMAGE_PATH, \"grid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c7b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define AUTOTUNE object\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def random_crop(lrImage, hrImage, hrCropSize=96, scale=4):\n",
    "    # calculate the low resolution image crop size and image shape\n",
    "    lrCropSize = hrCropSize // scale\n",
    "    lrImageShape = tf.shape(lrImage)[:2]\n",
    "    \n",
    "    # calculate the low resolution image width and height offsets\n",
    "    lrW = tf.random.uniform(shape=(),\n",
    "        maxval=lrImageShape[1] - lrCropSize + 1, dtype=tf.int32)\n",
    "    lrH = tf.random.uniform(shape=(),\n",
    "        maxval=lrImageShape[0] - lrCropSize + 1, dtype=tf.int32)\n",
    "    \n",
    "    # calculate the high resolution image width and height\n",
    "    hrW = lrW * scale\n",
    "    hrH = lrH * scale\n",
    "    \n",
    "    # crop the low and high resolution images\n",
    "    lrImageCropped = tf.slice(lrImage, [lrH, lrW, 0], \n",
    "        [(lrCropSize), (lrCropSize), 3])\n",
    "    hrImageCropped = tf.slice(hrImage, [hrH, hrW, 0],\n",
    "        [(hrCropSize), (hrCropSize), 3])\n",
    "    \n",
    "    # return the cropped low and high resolution images\n",
    "    return (lrImageCropped, hrImageCropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c6872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_crop(lrImage, hrImage, hrCropSize=96, scale=4):\n",
    "    # calculate the low resolution image crop size and image shape\n",
    "    lrCropSize = hrCropSize // scale\n",
    "    lrImageShape = tf.shape(lrImage)[:2]\n",
    "    \n",
    "    # calculate the low resolution image width and height\n",
    "    lrW = lrImageShape[1] // 2\n",
    "    lrH = lrImageShape[0] // 2\n",
    "    \n",
    "    # calculate the high resolution image width and height\n",
    "    hrW = lrW * scale\n",
    "    hrH = lrH * scale\n",
    "    \n",
    "    # crop the low and high resolution images\n",
    "    lrImageCropped = tf.slice(lrImage, [lrH - (lrCropSize // 2),\n",
    "        lrW - (lrCropSize // 2), 0], [lrCropSize, lrCropSize, 3])\n",
    "    hrImageCropped = tf.slice(hrImage, [hrH - (hrCropSize // 2),\n",
    "        hrW - (hrCropSize // 2), 0], [hrCropSize, hrCropSize, 3])\n",
    "    \n",
    "    # return the cropped low and high resolution images\n",
    "    return (lrImageCropped, hrImageCropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(lrImage, hrImage):\n",
    "    # calculate a random chance for flip\n",
    "    flipProb = tf.random.uniform(shape=(), maxval=1)\n",
    "    (lrImage, hrImage) = tf.cond(flipProb < 0.5,\n",
    "        lambda: (lrImage, hrImage),\n",
    "        lambda: (tf.image.flip_left_right(lrImage), tf.image.flip_left_right(hrImage)))\n",
    "    \n",
    "    # return the randomly flipped low and high resolution images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate(lrImage, hrImage):\n",
    "    # randomly generate the number of 90 degree rotations\n",
    "    n = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n",
    "    \n",
    "    # rotate the low and high resolution images\n",
    "    lrImage = tf.image.rot90(lrImage, n)\n",
    "    hrImage = tf.imagerot90(hrImage, n)\n",
    "    \n",
    "    # return the randomly rotated images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_example(example):\n",
    "    # get the feature template and  parse a single image according to\n",
    "    # the feature template\n",
    "    feature = {\n",
    "        \"lr\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"hr\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    \n",
    "    # parse the low and high resolution images\n",
    "    lrImage = tf.io.parse_tensor(example[\"lr\"], out_type=tf.uint8)\n",
    "    hrImage = tf.io.parse_tensor(example[\"hr\"], out_type=tf.uint8)\n",
    "    \n",
    "    # perform data augmentation\n",
    "    (lrImage, hrImage) = random_crop(lrImage, hrImage)\n",
    "    (lrImage, hrImage) = random_flip(lrImage, hrImage)\n",
    "    (lrImage, hrImage) = random_rotate(lrImage, hrImage)\n",
    "    \n",
    "    # reshape the low and high resolution images\n",
    "    lrImage = tf.reshape(lrImage, (24, 24, 3))\n",
    "    hrImage = tf.reshape(hrImage, (96, 96, 3))\n",
    "    \n",
    "    # return the low and high resolution images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_example(example):\n",
    "    # get the feature template and  parse a single image according to\n",
    "    # the feature template\n",
    "    feature = {\n",
    "        \"lr\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"hr\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    \n",
    "    # parse the low and high resolution images\n",
    "    lrImage = tf.io.parse_tensor(example[\"lr\"], out_type=tf.uint8)\n",
    "    hrImage = tf.io.parse_tensor(example[\"hr\"], out_type=tf.uint8)\n",
    "    \n",
    "    # center crop both low and high resolution image\n",
    "    (lrImage, hrImage) = get_center_crop(lrImage, hrImage)\n",
    "    \n",
    "    # reshape the low and high resolution images\n",
    "    lrImage = tf.reshape(lrImage, (24, 24, 3))\n",
    "    hrImage = tf.reshape(hrImage, (96, 96, 3))\n",
    "    \n",
    "    # return the low and high resolution images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3061035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, batchSize, train=False):\n",
    "    # get the TFRecords from the filenames\n",
    "    dataset = tf.data.TFRecordDataset(filenames, \n",
    "        num_parallel_reads=AUTO)\n",
    "    \n",
    "    # check if this is the training dataset\n",
    "    if train:\n",
    "        # read the training examples\n",
    "        dataset = dataset.map(read_train_example,\n",
    "            num_parallel_calls=AUTO)\n",
    "    # otherwise, we are working with the test dataset\n",
    "    else:\n",
    "        # read the test examples\n",
    "        dataset = dataset.map(read_test_example,\n",
    "            num_parallel_calls=AUTO)\n",
    "        \n",
    "    # batch and prefetch the data\n",
    "    dataset = (dataset\n",
    "        .shuffle(batchSize)\n",
    "        .batch(batchSize)\n",
    "        .repeat()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    \n",
    "    # return the dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a34a7",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses:\n",
    "    \n",
    "    def __init__(self, numReplicas):\n",
    "        self.numReplicas = numReplicas\n",
    "        \n",
    "    def bce_loss(self, real, pred):\n",
    "        # compute binary cross entropy loss without reduction\n",
    "        BCE = BinaryCrossentropy(reduction=Reduction.NONE)\n",
    "        loss = BCE(real, pred)\n",
    "        \n",
    "        # compute reduced mean over the entire batch\n",
    "        loss = reduce_mean(loss) * (1. / self.numReplicas)\n",
    "        \n",
    "        # return reduced bce loss\n",
    "        return loss\n",
    "    \n",
    "    def mse_loss(self, real, pred):\n",
    "        # compute mean squared error loss without reduction\n",
    "        MSE = MeanSquaredError(reduction=Reduction.NONE)\n",
    "        loss = MSE(real, pred)\n",
    "        \n",
    "        # compute reduced mean over the entire batch\n",
    "        loss = reduce_mean(loss) * (1. / self.numReplicas)\n",
    "        \n",
    "        # return reduced mse loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778230b",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def generator(scalingFactor, featureMaps, residualBlocks):\n",
    "        # initialize the input layer\n",
    "        inputs = Input((None, None, 3))\n",
    "        xIn = Rescaling(scale=(1.0 / 255.0), offset=0.0)(inputs)\n",
    "        \n",
    "        # pass the input through CONV => PReLU block\n",
    "        xIn = Conv2D(featureMaps, 9, padding=\"same\")(xIn)\n",
    "        xIn = PReLU(shared_axes=[1, 2])(xIn)\n",
    "        \n",
    "        # construct the \"residual in residual\" block\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(xIn)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        xSkip = Add()([xIn, x])\n",
    "        \n",
    "        # create a number of residual blocks\n",
    "        for _ in range(residualBlocks - 1):\n",
    "            x = Conv2D(featureMaps, 3, padding=\"same\")(xSkip)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = PReLU(shared_axes=[1, 2])(x)\n",
    "            x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            xSkip = Add()([xSkip, x])\n",
    "        \n",
    "        # get the last residual block without activation\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(xSkip)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([xIn, x])\n",
    "        \n",
    "        # upscale the image with pixel shuffle\n",
    "        x = Conv2D(featureMaps * (scalingFactor // 2), 3, padding=\"same\")(x)\n",
    "        x = tf.nn.depth_to_space(x, 2)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "        \n",
    "        # upscale the image with pixel shuffle\n",
    "        x = Conv2D(featureMaps * scalingFactor, 3,\n",
    "            padding=\"same\")(x)\n",
    "        x = tf.nn.depth_to_space(x, 2)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "        \n",
    "        # get the output and scale it from [-1, 1] to [0, 255] range\n",
    "        x = Conv2D(3, 9, padding=\"same\", activation=\"tanh\")(x)\n",
    "        x = Rescaling(scale=127.5, offset=127.5)(x)\n",
    "    \n",
    "        # create the generator model\n",
    "        generator = Model(inputs, x)\n",
    "        \n",
    "        # return the generator\n",
    "        return generator\n",
    "    \n",
    "    @staticmethod\n",
    "    def discriminator(featureMaps, leakyAlpha, discBlocks):\n",
    "        # initialize the input layer and process it with conv kernel\n",
    "        inputs = Input((None, None, 3))\n",
    "        x = Rescaling(scale=(1.0 / 127.5), offset=-1.0)(inputs)\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "        \n",
    "        # unlike the generator we use leaky relu in the discriminator\n",
    "        x = LeakyReLU(leakyAlpha)(x)\n",
    "        \n",
    "        # pass the output from previous layer through a CONV => BN =>\n",
    "        # LeakyReLU block\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(leakyAlpha)(x)\n",
    "        \n",
    "        # create a number of discriminator blocks\n",
    "        for i in range(1, discBlocks):\n",
    "            # first CONV => BN => LeakyReLU block\n",
    "            x = Conv2D(featureMaps * (2 ** i), 3, strides=2,\n",
    "                padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(leakyAlpha)(x)\n",
    "            # second CONV => BN => LeakyReLU block\n",
    "            x = Conv2D(featureMaps * (2 ** i), 3, padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(leakyAlpha)(x)\n",
    "            \n",
    "        # process the feature maps with global average pooling\n",
    "        x = GlobalAvgPool2D()(x)\n",
    "        x = LeakyReLU(leakyAlpha)(x)\n",
    "        \n",
    "        # final FC layer with sigmoid activation function\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        \n",
    "        # create the discriminator model\n",
    "        discriminator = Model(inputs, x)\n",
    "        \n",
    "        # return the discriminator\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed415647",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f027047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGANTraining(Model):\n",
    "    \n",
    "    def __init__(self, generator, discriminator, vgg, batchSize):\n",
    "        super().__init__()\n",
    "        # initialize the generator, discriminator, vgg model, and \n",
    "        # the global batch size\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.vgg = vgg\n",
    "        self.batchSize = batchSize\n",
    "    \n",
    "    def compile(self, gOptimizer, dOptimizer, bceLoss, mseLoss):\n",
    "        super().compile()\n",
    "        # initialize the optimizers for the generator \n",
    "        # and discriminator\n",
    "        self.gOptimizer = gOptimizer\n",
    "        self.dOptimizer = dOptimizer\n",
    "        \n",
    "        # initialize the loss functions\n",
    "        self.bceLoss = bceLoss\n",
    "        self.mseLoss = mseLoss\n",
    "    \n",
    "    def train_step(self, images):\n",
    "        # grab the low and high resolution images\n",
    "        (lrImages, hrImages) = images\n",
    "        lrImages = tf.cast(lrImages, tf.float32)\n",
    "        hrImages = tf.cast(hrImages, tf.float32)\n",
    "        \n",
    "        # generate super resolution images\n",
    "        srImages = self.generator(lrImages)\n",
    "        \n",
    "        # combine them with real images\n",
    "        combinedImages = concat([srImages, hrImages], axis=0)\n",
    "        \n",
    "        # assemble labels discriminating real from fake images where\n",
    "        # label 0 is for predicted images and 1 is for original high\n",
    "        # resolution images\n",
    "        labels = concat(\n",
    "            [zeros((self.batchSize, 1)), ones((self.batchSize, 1))],\n",
    "            axis=0)\n",
    "        \n",
    "        # train the discriminator\n",
    "        with GradientTape() as tape:\n",
    "            # get the discriminator predictions\n",
    "            predictions = self.discriminator(combinedImages)\n",
    "            \n",
    "            # compute the loss\n",
    "            dLoss = self.bceLoss(labels, predictions)\n",
    "        \n",
    "        # compute the gradients\n",
    "        grads = tape.gradient(dLoss,\n",
    "            self.discriminator.trainable_variables)\n",
    "        \n",
    "        # optimize the discriminator weights according to the\n",
    "        # gradients computed\n",
    "        self.dOptimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # generate misleading labels\n",
    "        misleadingLabels = ones((self.batchSize, 1))\n",
    "        \n",
    "        # train the generator (note that we should *not* update the\n",
    "        #  weights of the discriminator)!\n",
    "        with GradientTape() as tape:\n",
    "            # get fake images from the generator\n",
    "            fakeImages = self.generator(lrImages)\n",
    "        \n",
    "            # get the prediction from the discriminator\n",
    "            predictions = self.discriminator(fakeImages)\n",
    "        \n",
    "            # compute the adversarial loss\n",
    "            gLoss = 1e-3 * self.bceLoss(misleadingLabels, predictions)\n",
    "            \n",
    "            # compute the normalized vgg outputs\n",
    "            srVgg = tf.keras.applications.vgg19.preprocess_input(\n",
    "                fakeImages)\n",
    "            srVgg = self.vgg(srVgg) / 12.75\n",
    "            hrVgg = tf.keras.applications.vgg19.preprocess_input(\n",
    "                hrImages)\n",
    "            hrVgg = self.vgg(hrVgg) / 12.75\n",
    "            # compute the perceptual loss\n",
    "            percLoss = self.mseLoss(hrVgg, srVgg)\n",
    "        \n",
    "            # calculate the total generator loss\n",
    "            gTotalLoss = gLoss + percLoss\n",
    "        \n",
    "        # compute the gradients\n",
    "        grads = tape.gradient(gTotalLoss,\n",
    "            self.generator.trainable_variables)\n",
    "        \n",
    "        # optimize the generator weights with the computed gradients\n",
    "        self.gOptimizer.apply_gradients(zip(grads,\n",
    "            self.generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # return the generator and discriminator losses\n",
    "        return {\"dLoss\": dLoss, \"gTotalLoss\": gTotalLoss,\n",
    "            \"gLoss\": gLoss, \"percLoss\": percLoss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc394889",
   "metadata": {},
   "source": [
    "## Implementing the Final Utility Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG:\n",
    "    \n",
    "    @staticmethod\n",
    "    def build():\n",
    "        # initialize the pre-trained VGG19 model\n",
    "        vgg = VGG19(input_shape=(None, None, 3), weights=\"imagenet\",\n",
    "            include_top=False)\n",
    "        \n",
    "        # slicing the VGG19 model till layer #20\n",
    "        model = Model(vgg.input, vgg.layers[20].output)\n",
    "        \n",
    "        # return the sliced VGG19 model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a4014",
   "metadata": {},
   "source": [
    "## Assesing the output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code snippet has been taken from:\n",
    "# https://keras.io/examples/vision/super_resolution_sub_pixel\n",
    "def zoom_into_images(image, imageTitle):\n",
    "    # create a new figure with a default 111 subplot.\n",
    "    (fig, ax) = subplots()\n",
    "    im = ax.imshow(array_to_img(image[::-1]), origin=\"lower\")\n",
    "    title(imageTitle)\n",
    "    \n",
    "    # zoom-factor: 2.0, location: upper-left\n",
    "    axins = zoomed_inset_axes(ax, 2, loc=2)\n",
    "    axins.imshow(array_to_img(image[::-1]), origin=\"lower\")\n",
    "    \n",
    "    # specify the limits.\n",
    "    (x1, x2, y1, y2) = 20, 40, 20, 40\n",
    "    \n",
    "    # apply the x-limits.\n",
    "    axins.set_xlim(x1, x2)\n",
    "    \n",
    "    # apply the y-limits.\n",
    "    axins.set_ylim(y1, y2)\n",
    "    \n",
    "    # remove the xticks and yticks\n",
    "    yticks(visible=False)\n",
    "    xticks(visible=False)\n",
    "    \n",
    "    # make the line.\n",
    "    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n",
    "    \n",
    "    # build the image path and save it to disk\n",
    "    imagePath = os.path.join(BASE_IMAGE_PATH, f\"{imageTitle}.png\")\n",
    "    savefig(imagePath)\n",
    "    \n",
    "    # show the image\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f95cd0",
   "metadata": {},
   "source": [
    "## Training the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# define the multi-gpu strategy\n",
    "strategy = distribute.MirroredStrategy()\n",
    "\n",
    "# set the train TFRecords, pretrained generator, and final\n",
    "# generator model paths to be used for GPU training\n",
    "tfrTrainPath = GPU_DIV2K_TFR_TRAIN_PATH\n",
    "pretrainedGenPath = GPU_PRETRAINED_GENERATOR_MODEL\n",
    "genPath = GPU_GENERATOR_MODEL\n",
    "\n",
    "# display the number of accelerators\n",
    "print(\"[INFO] number of accelerators: {}...\"\n",
    "    .format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# grab train TFRecord filenames\n",
    "print(\"[INFO] grabbing the train TFRecords...\")\n",
    "trainTfr = glob(tfrTrainPath +\"/*.tfrec\")\n",
    "\n",
    "# build the div2k datasets from the TFRecords\n",
    "print(\"[INFO] creating train and test dataset...\")\n",
    "trainDs = load_dataset(filenames=trainTfr, train=True,\n",
    "    batchSize=TRAIN_BATCH_SIZE * strategy.num_replicas_in_sync)\n",
    "\n",
    "# call the strategy scope context manager\n",
    "with strategy.scope():\n",
    "    # initialize our losses class object\n",
    "    losses = Losses(numReplicas=strategy.num_replicas_in_sync)\n",
    "    \n",
    "    # initialize the generator, and compile it with Adam optimizer and\n",
    "    # MSE loss\n",
    "    generator = SRGAN.generator(\n",
    "        scalingFactor=SCALING_FACTOR,\n",
    "        featureMaps=FEATURE_MAPS,\n",
    "        residualBlocks=RESIDUAL_BLOCKS)\n",
    "    \n",
    "    generator.compile(\n",
    "        optimizer=Adam(learning_rate=PRETRAIN_LR),\n",
    "        loss=losses.mse_loss)\n",
    "    \n",
    "    # pretraining the generator\n",
    "    print(\"[INFO] pretraining SRGAN generator...\")\n",
    "    generator.fit(trainDs, epochs=PRETRAIN_EPOCHS,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH)\n",
    "    \n",
    "# check whether output model directory exists, if it doesn't, then\n",
    "# create it\n",
    "if not os.path.exists(BASE_OUTPUT_PATH):\n",
    "    os.makedirs(BASE_OUTPUT_PATH)\n",
    "    \n",
    "# save the pretrained generator\n",
    "print(\"[INFO] saving the SRGAN pretrained generator to {}...\"\n",
    "    .format(pretrainedGenPath))\n",
    "generator.save(pretrainedGenPath)\n",
    "\n",
    "# call the strategy scope context manager\n",
    "with strategy.scope():\n",
    "    # initialize our losses class object\n",
    "    losses = Losses(numReplicas=strategy.num_replicas_in_sync)\n",
    "    \n",
    "    # initialize the vgg network (for perceptual loss) and discriminator\n",
    "    # network\n",
    "    vgg = VGG.build()\n",
    "    \n",
    "    discriminator = SRGAN.discriminator(\n",
    "        featureMaps=FEATURE_MAPS, \n",
    "        leakyAlpha=LEAKY_ALPHA, discBlocks=DISC_BLOCKS)\n",
    "    \n",
    "    # build the SRGAN training model and compile it\n",
    "    srgan = SRGANTraining(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        vgg=vgg,\n",
    "        batchSize=TRAIN_BATCH_SIZE)\n",
    "    \n",
    "    srgan.compile(\n",
    "        dOptimizer=Adam(learning_rate=FINETUNE_LR),\n",
    "        gOptimizer=Adam(learning_rate=FINETUNE_LR),\n",
    "        bceLoss=losses.bce_loss,\n",
    "        mseLoss=losses.mse_loss,\n",
    "    )\n",
    "    \n",
    "    # train the SRGAN model\n",
    "    print(\"[INFO] training SRGAN...\")\n",
    "    srgan.fit(trainDs, epochs=FINETUNE_EPOCHS,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH)\n",
    "\n",
    "# save the SRGAN generator\n",
    "print(\"[INFO] saving SRGAN generator to {}...\".format(genPath))\n",
    "srgan.generator.save(genPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be62b43",
   "metadata": {},
   "source": [
    "## Creating the inference script for the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the multi-gpu strategy\n",
    "strategy = distribute.MirroredStrategy()\n",
    "\n",
    "# set the train TFRecords, pretrained generator, and final\n",
    "# generator model paths to be used for GPU training\n",
    "tfrTestPath = GPU_DIV2K_TFR_TEST_PATH\n",
    "pretrainedGenPath = GPU_PRETRAINED_GENERATOR_MODEL\n",
    "genPath = GPU_GENERATOR_MODEL\n",
    "\n",
    "# get the dataset\n",
    "print(\"[INFO] loading the test dataset...\")\n",
    "testTfr = glob(tfrTestPath + \"/*.tfrec\")\n",
    "testDs = load_dataset(testTfr, INFER_BATCH_SIZE, train=False)\n",
    "\n",
    "# get the first batch of testing images\n",
    "(lrImage, hrImage) = next(iter(testDs))\n",
    "\n",
    "# call the strategy scope context manager\n",
    "with strategy.scope(): \n",
    "    # load the SRGAN trained models\n",
    "    print(\"[INFO] loading the pre-trained and fully trained SRGAN model...\")\n",
    "    srganPreGen = load_model(pretrainedGenPath, compile=False)\n",
    "    srganGen = load_model(genPath, compile=False)\n",
    "    \n",
    "    # predict using SRGAN\n",
    "    print(\"[INFO] making predictions with pre-trained and fully trained SRGAN model...\")\n",
    "    srganPreGenPred = srganPreGen.predict(lrImage)\n",
    "    srganGenPred = srganGen.predict(lrImage)\n",
    "\n",
    "# plot the respective predictions\n",
    "print(\"[INFO] plotting the SRGAN predictions...\")\n",
    "(fig, axes) = subplots(nrows=INFER_BATCH_SIZE, ncols=4,\n",
    "    figsize=(50, 50))\n",
    "\n",
    "# plot the predicted images from low res to high res\n",
    "for (ax, lowRes, srPreIm, srGanIm, highRes) in zip(axes, lrImage,\n",
    "        srganPreGenPred, srganGenPred, hrImage):\n",
    "    # plot the low resolution image\n",
    "    ax[0].imshow(array_to_img(lowRes))\n",
    "    ax[0].set_title(\"Low Resolution Image\")\n",
    "    \n",
    "    # plot the pretrained SRGAN image\n",
    "    ax[1].imshow(array_to_img(srPreIm))\n",
    "    ax[1].set_title(\"SRGAN Pretrained\")\n",
    "    \n",
    "    # plot the SRGAN image\n",
    "    ax[2].imshow(array_to_img(srGanIm))\n",
    "    ax[2].set_title(\"SRGAN\")\n",
    "    \n",
    "    # plot the high resolution image\n",
    "    ax[3].imshow(array_to_img(highRes))\n",
    "    ax[3].set_title(\"High Resolution Image\")\n",
    "\n",
    "# check whether output image directory exists, if it doesn't, then\n",
    "# create it\n",
    "if not os.path.exists(BASE_IMAGE_PATH):\n",
    "    os.makedirs(BASE_IMAGE_PATH)\n",
    "    \n",
    "# serialize the results to disk\n",
    "print(\"[INFO] saving the SRGAN predictions to disk...\")\n",
    "fig.savefig(GRID_IMAGE_PATH)\n",
    "\n",
    "# plot the zoomed in images\n",
    "zoom_into_images(srganPreGenPred[0], \"SRGAN Pretrained\")\n",
    "zoom_into_images(srganGenPred[0], \"SRGAN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
