{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b1c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, Reduction\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, GlobalAvgPool2D, LeakyReLU, Rescaling,\n",
    "    Conv2D, Dense, PReLU, Add, Input\n",
    ")\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow import GradientTape, concat, zeros, ones, reduce_mean, distribute\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from matplotlib.pyplot import subplots, savefig, title, xticks, yticks, show\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from tensorflow.config import experimental_connect_to_cluster\n",
    "from tensorflow.tpu.experimental import initialize_tpu_system\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.io.gfile import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6a90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the TFDS dataset we will be using\n",
    "DATASET = \"div2k/bicubic_x4\"\n",
    "\n",
    "# define the shard size and batch size\n",
    "SHARD_SIZE = 256\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "INFER_BATCH_SIZE = 8\n",
    "\n",
    "# dataset specs\n",
    "HR_SHAPE = [96, 96, 3]\n",
    "LR_SHAPE = [24, 24, 3]\n",
    "SCALING_FACTOR = 4\n",
    "\n",
    "# GAN model specs\n",
    "FEATURE_MAPS = 64\n",
    "RESIDUAL_BLOCKS = 16\n",
    "LEAKY_ALPHA = 0.2\n",
    "DISC_BLOCKS = 4\n",
    "\n",
    "# training specs\n",
    "PRETRAIN_LR = 1e-4\n",
    "FINETUNE_LR = 1e-5\n",
    "PRETRAIN_EPOCHS = 2500\n",
    "FINETUNE_EPOCHS = 2500\n",
    "STEPS_PER_EPOCH = 10\n",
    "\n",
    "# define the path to the dataset\n",
    "BASE_DATA_PATH = \"dataset\"\n",
    "DIV2K_PATH = os.path.join(BASE_DATA_PATH, \"div2k\")\n",
    "\n",
    "# define the path to the tfrecords for GPU training\n",
    "GPU_BASE_TFR_PATH = \"tfrecord\"\n",
    "GPU_DIV2K_TFR_TRAIN_PATH = os.path.join(GPU_BASE_TFR_PATH, \"train\")\n",
    "GPU_DIV2K_TFR_TEST_PATH = os.path.join(GPU_BASE_TFR_PATH, \"test\")\n",
    "\n",
    "# path to our base output directory\n",
    "BASE_OUTPUT_PATH = \"outputs\"\n",
    "\n",
    "# GPU training SRGAN model paths\n",
    "GPU_PRETRAINED_GENERATOR_MODEL = os.path.join(BASE_OUTPUT_PATH,\n",
    "    \"models\", \"pretrained_generator\")\n",
    "GPU_GENERATOR_MODEL = os.path.join(BASE_OUTPUT_PATH, \"models\",\n",
    "    \"generator\")\n",
    "\n",
    "# define the path to the inferred images and to the grid image\n",
    "BASE_IMAGE_PATH = os.path.join(BASE_OUTPUT_PATH, \"images\")\n",
    "GRID_IMAGE_PATH = os.path.join(BASE_IMAGE_PATH, \"grid.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c7b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define AUTOTUNE object\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def random_crop(lrImage, hrImage, hrCropSize=96, scale=4):\n",
    "    # calculate the low resolution image crop size and image shape\n",
    "    lrCropSize = hrCropSize // scale\n",
    "    lrImageShape = tf.shape(lrImage)[:2]\n",
    "    \n",
    "    # calculate the low resolution image width and height offsets\n",
    "    lrW = tf.random.uniform(shape=(),\n",
    "        maxval=lrImageShape[1] - lrCropSize + 1, dtype=tf.int32)\n",
    "    lrH = tf.random.uniform(shape=(),\n",
    "        maxval=lrImageShape[0] - lrCropSize + 1, dtype=tf.int32)\n",
    "    \n",
    "    # calculate the high resolution image width and height\n",
    "    hrW = lrW * scale\n",
    "    hrH = lrH * scale\n",
    "    \n",
    "    # crop the low and high resolution images\n",
    "    lrImageCropped = tf.slice(lrImage, [lrH, lrW, 0], \n",
    "        [(lrCropSize), (lrCropSize), 3])\n",
    "    hrImageCropped = tf.slice(hrImage, [hrH, hrW, 0],\n",
    "        [(hrCropSize), (hrCropSize), 3])\n",
    "    \n",
    "    # return the cropped low and high resolution images\n",
    "    return (lrImageCropped, hrImageCropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c6872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_crop(lrImage, hrImage, hrCropSize=96, scale=4):\n",
    "    # calculate the low resolution image crop size and image shape\n",
    "    lrCropSize = hrCropSize // scale\n",
    "    lrImageShape = tf.shape(lrImage)[:2]\n",
    "    \n",
    "    # calculate the low resolution image width and height\n",
    "    lrW = lrImageShape[1] // 2\n",
    "    lrH = lrImageShape[0] // 2\n",
    "    \n",
    "    # calculate the high resolution image width and height\n",
    "    hrW = lrW * scale\n",
    "    hrH = lrH * scale\n",
    "    \n",
    "    # crop the low and high resolution images\n",
    "    lrImageCropped = tf.slice(lrImage, [lrH - (lrCropSize // 2),\n",
    "        lrW - (lrCropSize // 2), 0], [lrCropSize, lrCropSize, 3])\n",
    "    hrImageCropped = tf.slice(hrImage, [hrH - (hrCropSize // 2),\n",
    "        hrW - (hrCropSize // 2), 0], [hrCropSize, hrCropSize, 3])\n",
    "    \n",
    "    # return the cropped low and high resolution images\n",
    "    return (lrImageCropped, hrImageCropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dfeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(lrImage, hrImage):\n",
    "    # calculate a random chance for flip\n",
    "    flipProb = tf.random.uniform(shape=(), maxval=1)\n",
    "    (lrImage, hrImage) = tf.cond(flipProb < 0.5,\n",
    "        lambda: (lrImage, hrImage),\n",
    "        lambda: (tf.image.flip_left_right(lrImage), tf.image.flip_left_right(hrImage)))\n",
    "    \n",
    "    # return the randomly flipped low and high resolution images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1f4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate(lrImage, hrImage):\n",
    "    # randomly generate the number of 90 degree rotations\n",
    "    n = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n",
    "    \n",
    "    # rotate the low and high resolution images\n",
    "    lrImage = tf.image.rot90(lrImage, n)\n",
    "    hrImage = tf.imagerot90(hrImage, n)\n",
    "    \n",
    "    # return the randomly rotated images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad8b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_example(example):\n",
    "    # get the feature template and  parse a single image according to\n",
    "    # the feature template\n",
    "    feature = {\n",
    "        \"lr\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"hr\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    \n",
    "    # parse the low and high resolution images\n",
    "    lrImage = tf.io.parse_tensor(example[\"lr\"], out_type=tf.uint8)\n",
    "    hrImage = tf.io.parse_tensor(example[\"hr\"], out_type=tf.uint8)\n",
    "    \n",
    "    # perform data augmentation\n",
    "    (lrImage, hrImage) = random_crop(lrImage, hrImage)\n",
    "    (lrImage, hrImage) = random_flip(lrImage, hrImage)\n",
    "    (lrImage, hrImage) = random_rotate(lrImage, hrImage)\n",
    "    \n",
    "    # reshape the low and high resolution images\n",
    "    lrImage = tf.reshape(lrImage, (24, 24, 3))\n",
    "    hrImage = tf.reshape(hrImage, (96, 96, 3))\n",
    "    \n",
    "    # return the low and high resolution images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc3dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_example(example):\n",
    "    # get the feature template and  parse a single image according to\n",
    "    # the feature template\n",
    "    feature = {\n",
    "        \"lr\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"hr\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    \n",
    "    # parse the low and high resolution images\n",
    "    lrImage = tf.io.parse_tensor(example[\"lr\"], out_type=tf.uint8)\n",
    "    hrImage = tf.io.parse_tensor(example[\"hr\"], out_type=tf.uint8)\n",
    "    \n",
    "    # center crop both low and high resolution image\n",
    "    (lrImage, hrImage) = get_center_crop(lrImage, hrImage)\n",
    "    \n",
    "    # reshape the low and high resolution images\n",
    "    lrImage = tf.reshape(lrImage, (24, 24, 3))\n",
    "    hrImage = tf.reshape(hrImage, (96, 96, 3))\n",
    "    \n",
    "    # return the low and high resolution images\n",
    "    return (lrImage, hrImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3061035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, batchSize, train=False):\n",
    "    # get the TFRecords from the filenames\n",
    "    dataset = tf.data.TFRecordDataset(filenames, \n",
    "        num_parallel_reads=AUTO)\n",
    "    \n",
    "    # check if this is the training dataset\n",
    "    if train:\n",
    "        # read the training examples\n",
    "        dataset = dataset.map(read_train_example,\n",
    "            num_parallel_calls=AUTO)\n",
    "    # otherwise, we are working with the test dataset\n",
    "    else:\n",
    "        # read the test examples\n",
    "        dataset = dataset.map(read_test_example,\n",
    "            num_parallel_calls=AUTO)\n",
    "        \n",
    "    # batch and prefetch the data\n",
    "    dataset = (dataset\n",
    "        .shuffle(batchSize)\n",
    "        .batch(batchSize)\n",
    "        .repeat()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    \n",
    "    # return the dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a34a7",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d5ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses:\n",
    "    \n",
    "    def __init__(self, numReplicas):\n",
    "        self.numReplicas = numReplicas\n",
    "        \n",
    "    def bce_loss(self, real, pred):\n",
    "        # compute binary cross entropy loss without reduction\n",
    "        BCE = BinaryCrossentropy(reduction=Reduction.NONE)\n",
    "        loss = BCE(real, pred)\n",
    "        \n",
    "        # compute reduced mean over the entire batch\n",
    "        loss = reduce_mean(loss) * (1. / self.numReplicas)\n",
    "        \n",
    "        # return reduced bce loss\n",
    "        return loss\n",
    "    \n",
    "    def mse_loss(self, real, pred):\n",
    "        # compute mean squared error loss without reduction\n",
    "        MSE = MeanSquaredError(reduction=Reduction.NONE)\n",
    "        loss = MSE(real, pred)\n",
    "        \n",
    "        # compute reduced mean over the entire batch\n",
    "        loss = reduce_mean(loss) * (1. / self.numReplicas)\n",
    "        \n",
    "        # return reduced mse loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778230b",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593b2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def generator(scalingFactor, featureMaps, residualBlocks):\n",
    "        # initialize the input layer\n",
    "        inputs = Input((None, None, 3))\n",
    "        xIn = Rescaling(scale=(1.0 / 255.0), offset=0.0)(inputs)\n",
    "        \n",
    "        # pass the input through CONV => PReLU block\n",
    "        xIn = Conv2D(featureMaps, 9, padding=\"same\")(xIn)\n",
    "        xIn = PReLU(shared_axes=[1, 2])(xIn)\n",
    "        \n",
    "        # construct the \"residual in residual\" block\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(xIn)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        xSkip = Add()([xIn, x])\n",
    "        \n",
    "        # create a number of residual blocks\n",
    "        for _ in range(residualBlocks - 1):\n",
    "            x = Conv2D(featureMaps, 3, padding=\"same\")(xSkip)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = PReLU(shared_axes=[1, 2])(x)\n",
    "            x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            xSkip = Add()([xSkip, x])\n",
    "        \n",
    "        # get the last residual block without activation\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(xSkip)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([xIn, x])\n",
    "        \n",
    "        # upscale the image with pixel shuffle\n",
    "        x = Conv2D(featureMaps * (scalingFactor // 2), 3, padding=\"same\")(x)\n",
    "        x = tf.nn.depth_to_space(x, 2)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "        \n",
    "        # upscale the image with pixel shuffle\n",
    "        x = Conv2D(featureMaps * scalingFactor, 3,\n",
    "            padding=\"same\")(x)\n",
    "        x = tf.nn.depth_to_space(x, 2)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "        \n",
    "        # get the output and scale it from [-1, 1] to [0, 255] range\n",
    "        x = Conv2D(3, 9, padding=\"same\", activation=\"tanh\")(x)\n",
    "        x = Rescaling(scale=127.5, offset=127.5)(x)\n",
    "    \n",
    "        # create the generator model\n",
    "        generator = Model(inputs, x)\n",
    "        \n",
    "        # return the generator\n",
    "        return generator\n",
    "    \n",
    "    @staticmethod\n",
    "    def discriminator(featureMaps, leakyAlpha, discBlocks):\n",
    "        # initialize the input layer and process it with conv kernel\n",
    "        inputs = Input((None, None, 3))\n",
    "        x = Rescaling(scale=(1.0 / 127.5), offset=-1.0)(inputs)\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "        \n",
    "        # unlike the generator we use leaky relu in the discriminator\n",
    "        x = LeakyReLU(leakyAlpha)(x)\n",
    "        \n",
    "        # pass the output from previous layer through a CONV => BN =>\n",
    "        # LeakyReLU block\n",
    "        x = Conv2D(featureMaps, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(leakyAlpha)(x)\n",
    "        \n",
    "        # create a number of discriminator blocks\n",
    "        for i in range(1, discBlocks):\n",
    "            # first CONV => BN => LeakyReLU block\n",
    "            x = Conv2D(featureMaps * (2 ** i), 3, strides=2,\n",
    "                padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(leakyAlpha)(x)\n",
    "            # second CONV => BN => LeakyReLU block\n",
    "            x = Conv2D(featureMaps * (2 ** i), 3, padding=\"same\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(leakyAlpha)(x)\n",
    "            \n",
    "        # process the feature maps with global average pooling\n",
    "        x = GlobalAvgPool2D()(x)\n",
    "        x = LeakyReLU(leakyAlpha)(x)\n",
    "        \n",
    "        # final FC layer with sigmoid activation function\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        \n",
    "        # create the discriminator model\n",
    "        discriminator = Model(inputs, x)\n",
    "        \n",
    "        # return the discriminator\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed415647",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f027047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGANTraining(Model):\n",
    "    \n",
    "    def __init__(self, generator, discriminator, vgg, batchSize):\n",
    "        super().__init__()\n",
    "        # initialize the generator, discriminator, vgg model, and \n",
    "        # the global batch size\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.vgg = vgg\n",
    "        self.batchSize = batchSize\n",
    "    \n",
    "    def compile(self, gOptimizer, dOptimizer, bceLoss, mseLoss):\n",
    "        super().compile()\n",
    "        # initialize the optimizers for the generator \n",
    "        # and discriminator\n",
    "        self.gOptimizer = gOptimizer\n",
    "        self.dOptimizer = dOptimizer\n",
    "        \n",
    "        # initialize the loss functions\n",
    "        self.bceLoss = bceLoss\n",
    "        self.mseLoss = mseLoss\n",
    "    \n",
    "    def train_step(self, images):\n",
    "        # grab the low and high resolution images\n",
    "        (lrImages, hrImages) = images\n",
    "        lrImages = tf.cast(lrImages, tf.float32)\n",
    "        hrImages = tf.cast(hrImages, tf.float32)\n",
    "        \n",
    "        # generate super resolution images\n",
    "        srImages = self.generator(lrImages)\n",
    "        \n",
    "        # combine them with real images\n",
    "        combinedImages = concat([srImages, hrImages], axis=0)\n",
    "        \n",
    "        # assemble labels discriminating real from fake images where\n",
    "        # label 0 is for predicted images and 1 is for original high\n",
    "        # resolution images\n",
    "        labels = concat(\n",
    "            [zeros((self.batchSize, 1)), ones((self.batchSize, 1))],\n",
    "            axis=0)\n",
    "        \n",
    "        # train the discriminator\n",
    "        with GradientTape() as tape:\n",
    "            # get the discriminator predictions\n",
    "            predictions = self.discriminator(combinedImages)\n",
    "            \n",
    "            # compute the loss\n",
    "            dLoss = self.bceLoss(labels, predictions)\n",
    "        \n",
    "        # compute the gradients\n",
    "        grads = tape.gradient(dLoss,\n",
    "            self.discriminator.trainable_variables)\n",
    "        \n",
    "        # optimize the discriminator weights according to the\n",
    "        # gradients computed\n",
    "        self.dOptimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # generate misleading labels\n",
    "        misleadingLabels = ones((self.batchSize, 1))\n",
    "        \n",
    "        # train the generator (note that we should *not* update the\n",
    "        #  weights of the discriminator)!\n",
    "        with GradientTape() as tape:\n",
    "            # get fake images from the generator\n",
    "            fakeImages = self.generator(lrImages)\n",
    "        \n",
    "            # get the prediction from the discriminator\n",
    "            predictions = self.discriminator(fakeImages)\n",
    "        \n",
    "            # compute the adversarial loss\n",
    "            gLoss = 1e-3 * self.bceLoss(misleadingLabels, predictions)\n",
    "            \n",
    "            # compute the normalized vgg outputs\n",
    "            srVgg = tf.keras.applications.vgg19.preprocess_input(\n",
    "                fakeImages)\n",
    "            srVgg = self.vgg(srVgg) / 12.75\n",
    "            hrVgg = tf.keras.applications.vgg19.preprocess_input(\n",
    "                hrImages)\n",
    "            hrVgg = self.vgg(hrVgg) / 12.75\n",
    "            # compute the perceptual loss\n",
    "            percLoss = self.mseLoss(hrVgg, srVgg)\n",
    "        \n",
    "            # calculate the total generator loss\n",
    "            gTotalLoss = gLoss + percLoss\n",
    "        \n",
    "        # compute the gradients\n",
    "        grads = tape.gradient(gTotalLoss,\n",
    "            self.generator.trainable_variables)\n",
    "        \n",
    "        # optimize the generator weights with the computed gradients\n",
    "        self.gOptimizer.apply_gradients(zip(grads,\n",
    "            self.generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # return the generator and discriminator losses\n",
    "        return {\"dLoss\": dLoss, \"gTotalLoss\": gTotalLoss,\n",
    "            \"gLoss\": gLoss, \"percLoss\": percLoss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc394889",
   "metadata": {},
   "source": [
    "## Implementing the Final Utility Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f36f9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG:\n",
    "    \n",
    "    @staticmethod\n",
    "    def build():\n",
    "        # initialize the pre-trained VGG19 model\n",
    "        vgg = VGG19(input_shape=(None, None, 3), weights=\"imagenet\",\n",
    "            include_top=False)\n",
    "        \n",
    "        # slicing the VGG19 model till layer #20\n",
    "        model = Model(vgg.input, vgg.layers[20].output)\n",
    "        \n",
    "        # return the sliced VGG19 model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a4014",
   "metadata": {},
   "source": [
    "## Assesing the output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2118bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code snippet has been taken from:\n",
    "# https://keras.io/examples/vision/super_resolution_sub_pixel\n",
    "def zoom_into_images(image, imageTitle):\n",
    "    # create a new figure with a default 111 subplot.\n",
    "    (fig, ax) = subplots()\n",
    "    im = ax.imshow(array_to_img(image[::-1]), origin=\"lower\")\n",
    "    title(imageTitle)\n",
    "    \n",
    "    # zoom-factor: 2.0, location: upper-left\n",
    "    axins = zoomed_inset_axes(ax, 2, loc=2)\n",
    "    axins.imshow(array_to_img(image[::-1]), origin=\"lower\")\n",
    "    \n",
    "    # specify the limits.\n",
    "    (x1, x2, y1, y2) = 20, 40, 20, 40\n",
    "    \n",
    "    # apply the x-limits.\n",
    "    axins.set_xlim(x1, x2)\n",
    "    \n",
    "    # apply the y-limits.\n",
    "    axins.set_ylim(y1, y2)\n",
    "    \n",
    "    # remove the xticks and yticks\n",
    "    yticks(visible=False)\n",
    "    xticks(visible=False)\n",
    "    \n",
    "    # make the line.\n",
    "    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n",
    "    \n",
    "    # build the image path and save it to disk\n",
    "    imagePath = os.path.join(BASE_IMAGE_PATH, f\"{imageTitle}.png\")\n",
    "    savefig(imagePath)\n",
    "    \n",
    "    # show the image\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f95cd0",
   "metadata": {},
   "source": [
    "## Training the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7d0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "[INFO] number of accelerators: 1...\n",
      "[INFO] grabbing the train TFRecords...\n",
      "[INFO] creating train and test dataset...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\bgmanuel\\AppData\\Local\\Temp\\ipykernel_25040\\3618052482.py\", line 17, in read_train_example  *\n        (lrImage, hrImage) = random_rotate(lrImage, hrImage)\n    File \"C:\\Users\\bgmanuel\\AppData\\Local\\Temp\\ipykernel_25040\\1311609548.py\", line 7, in random_rotate  *\n        hrImage = tf.imagerot90(hrImage, n)\n\n    AttributeError: module 'tensorflow' has no attribute 'imagerot90'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# build the div2k datasets from the TFRecords\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] creating train and test dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m trainDs \u001b[38;5;241m=\u001b[39m load_dataset(filenames\u001b[38;5;241m=\u001b[39mtrainTfr, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m     batchSize\u001b[38;5;241m=\u001b[39mTRAIN_BATCH_SIZE \u001b[38;5;241m*\u001b[39m strategy\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# call the strategy scope context manager\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# initialize our losses class object\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(filenames, batchSize, train)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# check if this is the training dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# read the training examples\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(read_train_example,\n\u001b[0;32m     10\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mAUTO)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# otherwise, we are working with the test dataset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# read the test examples\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(read_test_example,\n\u001b[0;32m     15\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mAUTO)\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2341\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m map_op\u001b[38;5;241m.\u001b[39m_map_v2(\n\u001b[0;32m   2342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2343\u001b[0m     map_func,\n\u001b[0;32m   2344\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m   2345\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m   2346\u001b[0m     synchronous\u001b[38;5;241m=\u001b[39msynchronous,\n\u001b[0;32m   2347\u001b[0m     use_unbounded_threadpool\u001b[38;5;241m=\u001b[39muse_unbounded_threadpool,\n\u001b[0;32m   2348\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2349\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:57\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`synchronous` is not supported with `num_parallel_calls`, but\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `num_parallel_calls` was set to \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m       num_parallel_calls,\n\u001b[0;32m     56\u001b[0m   )\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     58\u001b[0m     input_dataset,\n\u001b[0;32m     59\u001b[0m     map_func,\n\u001b[0;32m     60\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m     61\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m     62\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m     use_unbounded_threadpool\u001b[38;5;241m=\u001b[39muse_unbounded_threadpool,\n\u001b[0;32m     64\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:202\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m structured_function\u001b[38;5;241m.\u001b[39mStructuredFunctionWrapper(\n\u001b[0;32m    203\u001b[0m     map_func,\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformation_name(),\n\u001b[0;32m    205\u001b[0m     dataset\u001b[38;5;241m=\u001b[39minput_dataset,\n\u001b[0;32m    206\u001b[0m     use_legacy_function\u001b[38;5;241m=\u001b[39muse_legacy_function)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m fn_factory()\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1256\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1255\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1257\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1226\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwargs, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1230\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    698\u001b[0m )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1060\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1059\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1060\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m wrapper_helper(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, ag_ctx)(\u001b[38;5;241m*\u001b[39mnested_args)\n\u001b[0;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:693\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    694\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegrjnfs5q.py:16\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__read_train_example\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m     14\u001b[0m lrImage, hrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(random_crop), (ag__\u001b[38;5;241m.\u001b[39mld(lrImage), ag__\u001b[38;5;241m.\u001b[39mld(hrImage)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m lrImage, hrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(random_flip), (ag__\u001b[38;5;241m.\u001b[39mld(lrImage), ag__\u001b[38;5;241m.\u001b[39mld(hrImage)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 16\u001b[0m lrImage, hrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(random_rotate), (ag__\u001b[38;5;241m.\u001b[39mld(lrImage), ag__\u001b[38;5;241m.\u001b[39mld(hrImage)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     17\u001b[0m lrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(lrImage), (\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m3\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     18\u001b[0m hrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(hrImage), (\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m3\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mc:\\Users\\bgmanuel\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:441\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    439\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    443\u001b[0m   _attach_error_metadata(e, converted_f)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileawq47jx6.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__random_rotate\u001b[1;34m(lrImage, hrImage)\u001b[0m\n\u001b[0;32m     10\u001b[0m n \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m(), maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), fscope)\n\u001b[0;32m     11\u001b[0m lrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mrot90, (ag__\u001b[38;5;241m.\u001b[39mld(lrImage), ag__\u001b[38;5;241m.\u001b[39mld(n)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 12\u001b[0m hrImage \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mimagerot90, (ag__\u001b[38;5;241m.\u001b[39mld(hrImage), ag__\u001b[38;5;241m.\u001b[39mld(n)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\bgmanuel\\AppData\\Local\\Temp\\ipykernel_25040\\3618052482.py\", line 17, in read_train_example  *\n        (lrImage, hrImage) = random_rotate(lrImage, hrImage)\n    File \"C:\\Users\\bgmanuel\\AppData\\Local\\Temp\\ipykernel_25040\\1311609548.py\", line 7, in random_rotate  *\n        hrImage = tf.imagerot90(hrImage, n)\n\n    AttributeError: module 'tensorflow' has no attribute 'imagerot90'\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# define the multi-gpu strategy\n",
    "strategy = distribute.MirroredStrategy()\n",
    "\n",
    "# set the train TFRecords, pretrained generator, and final\n",
    "# generator model paths to be used for GPU training\n",
    "tfrTrainPath = GPU_DIV2K_TFR_TRAIN_PATH\n",
    "pretrainedGenPath = GPU_PRETRAINED_GENERATOR_MODEL\n",
    "genPath = GPU_GENERATOR_MODEL\n",
    "\n",
    "# display the number of accelerators\n",
    "print(\"[INFO] number of accelerators: {}...\"\n",
    "    .format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# grab train TFRecord filenames\n",
    "print(\"[INFO] grabbing the train TFRecords...\")\n",
    "trainTfr = glob(tfrTrainPath +\"/*.tfrec\")\n",
    "\n",
    "# build the div2k datasets from the TFRecords\n",
    "print(\"[INFO] creating train and test dataset...\")\n",
    "trainDs = load_dataset(filenames=trainTfr, train=True,\n",
    "    batchSize=TRAIN_BATCH_SIZE * strategy.num_replicas_in_sync)\n",
    "\n",
    "# call the strategy scope context manager\n",
    "with strategy.scope():\n",
    "    # initialize our losses class object\n",
    "    losses = Losses(numReplicas=strategy.num_replicas_in_sync)\n",
    "    \n",
    "    # initialize the generator, and compile it with Adam optimizer and\n",
    "    # MSE loss\n",
    "    generator = SRGAN.generator(\n",
    "        scalingFactor=SCALING_FACTOR,\n",
    "        featureMaps=FEATURE_MAPS,\n",
    "        residualBlocks=RESIDUAL_BLOCKS)\n",
    "    \n",
    "    generator.compile(\n",
    "        optimizer=Adam(learning_rate=PRETRAIN_LR),\n",
    "        loss=losses.mse_loss)\n",
    "    \n",
    "    # pretraining the generator\n",
    "    print(\"[INFO] pretraining SRGAN generator...\")\n",
    "    generator.fit(trainDs, epochs=PRETRAIN_EPOCHS,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH)\n",
    "    \n",
    "# check whether output model directory exists, if it doesn't, then\n",
    "# create it\n",
    "if not os.path.exists(BASE_OUTPUT_PATH):\n",
    "    os.makedirs(BASE_OUTPUT_PATH)\n",
    "    \n",
    "# save the pretrained generator\n",
    "print(\"[INFO] saving the SRGAN pretrained generator to {}...\"\n",
    "    .format(pretrainedGenPath))\n",
    "generator.save(pretrainedGenPath)\n",
    "\n",
    "# call the strategy scope context manager\n",
    "with strategy.scope():\n",
    "    # initialize our losses class object\n",
    "    losses = Losses(numReplicas=strategy.num_replicas_in_sync)\n",
    "    \n",
    "    # initialize the vgg network (for perceptual loss) and discriminator\n",
    "    # network\n",
    "    vgg = VGG.build()\n",
    "    \n",
    "    discriminator = SRGAN.discriminator(\n",
    "        featureMaps=FEATURE_MAPS, \n",
    "        leakyAlpha=LEAKY_ALPHA, discBlocks=DISC_BLOCKS)\n",
    "    \n",
    "    # build the SRGAN training model and compile it\n",
    "    srgan = SRGANTraining(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        vgg=vgg,\n",
    "        batchSize=TRAIN_BATCH_SIZE)\n",
    "    \n",
    "    srgan.compile(\n",
    "        dOptimizer=Adam(learning_rate=FINETUNE_LR),\n",
    "        gOptimizer=Adam(learning_rate=FINETUNE_LR),\n",
    "        bceLoss=losses.bce_loss,\n",
    "        mseLoss=losses.mse_loss,\n",
    "    )\n",
    "    \n",
    "    # train the SRGAN model\n",
    "    print(\"[INFO] training SRGAN...\")\n",
    "    srgan.fit(trainDs, epochs=FINETUNE_EPOCHS,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH)\n",
    "\n",
    "# save the SRGAN generator\n",
    "print(\"[INFO] saving SRGAN generator to {}...\".format(genPath))\n",
    "srgan.generator.save(genPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be62b43",
   "metadata": {},
   "source": [
    "## Creating the inference script for the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the multi-gpu strategy\n",
    "strategy = distribute.MirroredStrategy()\n",
    "\n",
    "# set the train TFRecords, pretrained generator, and final\n",
    "# generator model paths to be used for GPU training\n",
    "tfrTestPath = GPU_DIV2K_TFR_TEST_PATH\n",
    "pretrainedGenPath = GPU_PRETRAINED_GENERATOR_MODEL\n",
    "genPath = GPU_GENERATOR_MODEL\n",
    "\n",
    "# get the dataset\n",
    "print(\"[INFO] loading the test dataset...\")\n",
    "testTfr = glob(tfrTestPath + \"/*.tfrec\")\n",
    "testDs = load_dataset(testTfr, INFER_BATCH_SIZE, train=False)\n",
    "\n",
    "# get the first batch of testing images\n",
    "(lrImage, hrImage) = next(iter(testDs))\n",
    "\n",
    "# call the strategy scope context manager\n",
    "with strategy.scope(): \n",
    "    # load the SRGAN trained models\n",
    "    print(\"[INFO] loading the pre-trained and fully trained SRGAN model...\")\n",
    "    srganPreGen = load_model(pretrainedGenPath, compile=False)\n",
    "    srganGen = load_model(genPath, compile=False)\n",
    "    \n",
    "    # predict using SRGAN\n",
    "    print(\"[INFO] making predictions with pre-trained and fully trained SRGAN model...\")\n",
    "    srganPreGenPred = srganPreGen.predict(lrImage)\n",
    "    srganGenPred = srganGen.predict(lrImage)\n",
    "\n",
    "# plot the respective predictions\n",
    "print(\"[INFO] plotting the SRGAN predictions...\")\n",
    "(fig, axes) = subplots(nrows=INFER_BATCH_SIZE, ncols=4,\n",
    "    figsize=(50, 50))\n",
    "\n",
    "# plot the predicted images from low res to high res\n",
    "for (ax, lowRes, srPreIm, srGanIm, highRes) in zip(axes, lrImage,\n",
    "        srganPreGenPred, srganGenPred, hrImage):\n",
    "    # plot the low resolution image\n",
    "    ax[0].imshow(array_to_img(lowRes))\n",
    "    ax[0].set_title(\"Low Resolution Image\")\n",
    "    \n",
    "    # plot the pretrained SRGAN image\n",
    "    ax[1].imshow(array_to_img(srPreIm))\n",
    "    ax[1].set_title(\"SRGAN Pretrained\")\n",
    "    \n",
    "    # plot the SRGAN image\n",
    "    ax[2].imshow(array_to_img(srGanIm))\n",
    "    ax[2].set_title(\"SRGAN\")\n",
    "    \n",
    "    # plot the high resolution image\n",
    "    ax[3].imshow(array_to_img(highRes))\n",
    "    ax[3].set_title(\"High Resolution Image\")\n",
    "\n",
    "# check whether output image directory exists, if it doesn't, then\n",
    "# create it\n",
    "if not os.path.exists(BASE_IMAGE_PATH):\n",
    "    os.makedirs(BASE_IMAGE_PATH)\n",
    "    \n",
    "# serialize the results to disk\n",
    "print(\"[INFO] saving the SRGAN predictions to disk...\")\n",
    "fig.savefig(GRID_IMAGE_PATH)\n",
    "\n",
    "# plot the zoomed in images\n",
    "zoom_into_images(srganPreGenPred[0], \"SRGAN Pretrained\")\n",
    "zoom_into_images(srganGenPred[0], \"SRGAN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
