{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f26f0a5",
   "metadata": {},
   "source": [
    "# VGG16 fine-tuning for defect detection\n",
    "\n",
    "This section builds a VGG16 model with ImageNet weights, lets you choose any valid input shape (HxWx3), and fine-tune only the last layers you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import VGG16\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "from SRModels.loading_methods import load_defects_dataset_as_patches\n",
    "from SRModels.data_augmentation import AdvancedAugmentGenerator\n",
    "from SRModels.constants import VGG_PATCH_SIZE, VGG_STRIDE, RANDOM_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7279558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedVGG16:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.trained = False\n",
    "\n",
    "    def setup_model(self,\n",
    "                    input_shape=(128, 128, 3),\n",
    "                    num_classes=2,\n",
    "                    train_last_n_layers=4,\n",
    "                    base_trainable=False,\n",
    "                    dropout_rate=0.2,\n",
    "                    l2_reg=0.0,\n",
    "                    learning_rate=1e-3,\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    from_pretrained=False,\n",
    "                    pretrained_path=None):\n",
    "        \"\"\"Set up the VGG16 classifier, either by loading a pretrained model or building a new one.\"\"\"\n",
    "        if from_pretrained:\n",
    "            if pretrained_path is None or not os.path.isfile(pretrained_path):\n",
    "                raise FileNotFoundError(f\"Pretrained model file not found at {pretrained_path}\")\n",
    "            self.model = load_model(pretrained_path)\n",
    "            self.trained = True\n",
    "            print(f\"Loaded pretrained model from {pretrained_path}\")\n",
    "        else:\n",
    "            self.build_vgg16(\n",
    "                input_shape=input_shape,\n",
    "                num_classes=num_classes,\n",
    "                train_last_n_layers=train_last_n_layers,\n",
    "                base_trainable=base_trainable,\n",
    "                dropout_rate=dropout_rate,\n",
    "                l2_reg=l2_reg,\n",
    "            )\n",
    "            self.compile(learning_rate=learning_rate, loss=loss)\n",
    "\n",
    "    def build_vgg16(self,\n",
    "                    input_shape=(128, 128, 3),\n",
    "                    num_classes=2,\n",
    "                    train_last_n_layers=4,\n",
    "                    base_trainable=False,\n",
    "                    dropout_rate=0.2,\n",
    "                    l2_reg=0.0):\n",
    "        \"\"\"\n",
    "        Build a VGG16-based model with ImageNet weights and a custom classification head.\n",
    "        \"\"\"\n",
    "        assert input_shape[-1] == 3, \"Input must have 3 channels (RGB).\"\n",
    "\n",
    "        # Load VGG16 base with ImageNet weights and no top\n",
    "        base = VGG16(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            input_shape=input_shape,\n",
    "        )\n",
    "\n",
    "        # Freeze all layers by default\n",
    "        base.trainable = False\n",
    "\n",
    "        # Optionally unfreeze last N layers\n",
    "        if base_trainable and train_last_n_layers > 0:\n",
    "            for layer in base.layers[-train_last_n_layers:]:\n",
    "                if not isinstance(layer, BatchNormalization):\n",
    "                    layer.trainable = True\n",
    "\n",
    "        # Build head\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = base(inputs, training=False)\n",
    "        x = GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "        if dropout_rate > 0:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "        kernel_reg = l2(l2_reg) if l2_reg > 0 else None\n",
    "        x = Dense(256, activation=\"relu\", kernel_regularizer=kernel_reg)(x)\n",
    "        x = Dropout(dropout_rate)(x) if dropout_rate > 0 else x\n",
    "\n",
    "        outputs = Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n",
    "        self.model = Model(inputs, outputs, name=\"vgg16_finetune\")\n",
    "\n",
    "    def compile(self, learning_rate=1e-3, loss=\"sparse_categorical_crossentropy\"):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model is not built yet.\")\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "        self.model.summary()\n",
    "\n",
    "    def fit(self,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            use_augmentation=True,\n",
    "            use_mix=True,\n",
    "            augment_validation=False):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model is not built yet.\")\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-7, verbose=1)\n",
    "        ]\n",
    "\n",
    "        if use_augmentation:\n",
    "            train_gen = AdvancedAugmentGenerator(X_train, y_train, batch_size=batch_size, shuffle=True, use_mix=use_mix)\n",
    "\n",
    "            if augment_validation:\n",
    "                val_gen = AdvancedAugmentGenerator(X_val, y_val, batch_size=batch_size, shuffle=False, use_mix=use_mix)\n",
    "                self.model.fit(\n",
    "                    train_gen,\n",
    "                    steps_per_epoch=len(train_gen),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=len(val_gen),\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "            else:\n",
    "                self.model.fit(\n",
    "                    train_gen,\n",
    "                    steps_per_epoch=len(train_gen),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "        else:\n",
    "            self.model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        if not self.trained:\n",
    "            raise RuntimeError(\"Model has not been trained.\")\n",
    "        results = self.model.evaluate(X_test, y_test)\n",
    "        print(f\"Loss: {results[0]:.4f}, Accuracy: {results[1]:.4f}\")\n",
    "        return results\n",
    "\n",
    "    def save(self, directory=\"models/VGG16\"):\n",
    "        if not self.trained:\n",
    "            raise RuntimeError(\"Cannot save an untrained model.\")\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        path = os.path.join(directory, f\"VGG16_{timestamp}.h5\")\n",
    "        self.model.save(path)\n",
    "        print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999f12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"../../data/images/HR\"))\n",
    "CLASS_LABELS_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../../data/images/class_labels_map.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee59aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X ->  High-resolution patches (model input)\n",
    "# y -> Class labels (target)\n",
    "X, y = load_defects_dataset_as_patches(HR_ROOT, patch_size=VGG_PATCH_SIZE, stride=VGG_STRIDE, class_map_path=CLASS_LABELS_PATH)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a4bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2560, 96, 96, 3), Y shape: (2560,)\n",
      "X_train shape: (1843, 96, 96, 3), y_train shape: (1843,)\n",
      "X_val shape: (205, 96, 96, 3), y_val shape: (205,)\n",
      "X_test shape: (512, 96, 96, 3), y_test shape: (512,)\n",
      "Class distribution: {0: 1920, 1: 640}\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {X.shape}, Y shape: {y.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Class distribution: {dict(zip(unique, counts))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1936f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FineTunedVGG16()\n",
    "\n",
    "model.setup_model(\n",
    "    input_shape=X.shape[1:],\n",
    "    num_classes=np.unique(y).shape[0],\n",
    "    train_last_n_layers=6,\n",
    "    base_trainable=True,\n",
    "    dropout_rate=0.3,\n",
    "    l2_reg=1e-4,\n",
    "    learning_rate=1e-3,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    from_pretrained=False,\n",
    "    pretrained_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844034ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16_finetune\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " gap (GlobalAveragePooling2D  (None, 512)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " gap (GlobalAveragePooling2D  (None, 512)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,846,530\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,846,530\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    use_augmentation=True,\n",
    "    use_mix=True,\n",
    "    augment_validation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff24df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = FineTunedVGG16()\n",
    "\n",
    "pretrained_model.setup_model(from_pretrained=True, pretrained_path=\"models/VGG16/SRCNN_20250624_221949.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
